{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc85384-5338-44b5-8cca-0a23ede41570",
   "metadata": {
    "tags": []
   },
   "source": [
    "# This code is an estimation for CL\n",
    "## Test early stopping & late stopping\n",
    "    - early stopping: 50 epoches\n",
    "    - late atopping:  200 epoches\n",
    "## Ensemble average of 30 times\n",
    "- bessel filter; $w_n = 0.01$, the frequence based on Nyquist rate (=sample_freq/2) is: $0.01*1000/2=5$\n",
    "- no standardization; no normalization\n",
    "- with L2\n",
    "- activation func: PReLU\n",
    "- No CP'(t), CP''(t) !!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a4dee-08ed-4cc7-97a7-c4aadbccc951",
   "metadata": {},
   "source": [
    "# Define which task to run:\n",
    "- task  = ['predict', 'cross_validation']\n",
    "    - 'cross_validation': for cross validation; has 7 folds\n",
    "    - 'predict': for test using parameters chose by cross validation; has 1 fold --> fold8\n",
    "- force = ['CL', 'CD']\n",
    "- filt  = ['bessel', 'no filter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11df159-4433-4a6d-884b-50f11da2a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'predict' \n",
    "force = 'CL'     \n",
    "filt = 'bessel' \n",
    "ensemble_num = 30\n",
    "\n",
    "if task == 'cross_validation':\n",
    "    folds = 7\n",
    "elif task == 'predict':\n",
    "    folds = 1\n",
    "else:\n",
    "    print('Error. Please define your task.') \n",
    "print('Define the task complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49f3f8-1d3d-442c-b8f8-a8f296a5d983",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa489563-03e0-4c6b-8e8f-5f8387e044df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, Sequential, regularizers\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.layers import PReLU\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # 使用第二，三块GPU \"2,3\"（从0开始）\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.signal import bessel\n",
    "from scipy.signal import filtfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41484e1c-280f-4f3f-aff5-201b0dc5cb3f",
   "metadata": {},
   "source": [
    "# define the font style in the figures\n",
    "- title, label, legend, text, math text\n",
    "- font: Times New Roman\n",
    "- font size: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae26553-fe71-4edb-8b64-0c122733636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size']   = 20 # 15\n",
    "\n",
    "mpl.rcParams['mathtext.fontset'] = 'custom'\n",
    "mpl.rcParams['mathtext.rm'] = 'Times New Roman'\n",
    "mpl.rcParams['mathtext.it'] = 'Times New Roman:italic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05e152-f289-44c9-b5fc-e9ab961da7e8",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab674201-fae0-4446-8418-fc8b41e7a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder path\n",
    "data_path = '../gust_dataframe.csv'\n",
    "\n",
    "# read the excel data\n",
    "gust_origin = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3660c-9326-43eb-a986-cf97278989c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gust_origin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48e9af-2019-41d3-ad3a-8fe202dbe203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the first column as index, i.e. set case_nn as index, for convenience\n",
    "gust_origin.set_index('case_number', inplace=True)\n",
    "print(gust_origin.shape)\n",
    "# check the first 5 columns\n",
    "gust_origin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d367b8f-ff73-4193-9de8-d51101f72212",
   "metadata": {},
   "source": [
    "# filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca46c3-539b-4355-bf59-d79ffebe6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bessel filter for every column; only filt CP\n",
    "if filt == 'bessel':\n",
    "    \n",
    "    gust_index   = gust_origin.index\n",
    "    gust_column  = gust_origin.columns\n",
    "    gust_no_filter = np.zeros((len(set(gust_index.to_list())),len(gust_origin.loc[['case_01']]),len(gust_origin.columns))) # (32,2500,18)\n",
    "    gust_filter    = np.zeros((len(set(gust_index.to_list())),len(gust_origin.loc[['case_01']]),len(gust_origin.columns)))\n",
    "    case_all_no = (np.linspace(0,31,32)).astype(int)\n",
    "    b, a = bessel(8, 0.01) # define the bessel filter; 8 is the filter order; 0.01 is the W_n (between 0 to 1)\n",
    "    \n",
    "    for i in case_all_no:      # i is the case number\n",
    "        case_numer = 'case_' + str(i+1).zfill(2)\n",
    "        gust_no_filter[i] = gust_origin.loc[[case_numer]].to_numpy()\n",
    "        for j in range(len(gust_origin.columns)-2):       # j is the CP and CL CD number\n",
    "            gust_filter[i][:,j] = filtfilt(b, a, gust_no_filter[i][:,j])  # bessel for CP\n",
    "        gust_filter[i][:,len(gust_origin.columns)-2] = gust_no_filter[i][:,len(gust_origin.columns)-2]  # no filter for CL\n",
    "        gust_filter[i][:,len(gust_origin.columns)-1] = gust_no_filter[i][:,len(gust_origin.columns)-1]  # no filter for CD\n",
    "    gust = pd.DataFrame(data=np.concatenate(gust_filter,axis=0), index=gust_index, columns=gust_column)\n",
    "    \n",
    "elif filt == 'no filter':\n",
    "    gust = gust_origin\n",
    "    \n",
    "else:\n",
    "    print('Error. Please define your filter.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5309f75-f04f-4e5d-b3b1-b768710d9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gust\n",
    "gust.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64eed4-58fd-49e8-8dfb-ef3730f69ea5",
   "metadata": {},
   "source": [
    "## check filter result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfee08-0f29-4cab-9881-f26a15494d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0.3 # period is 0.3s\n",
    "f = 1000 # frequency is 1000Hz\n",
    "t_star = np.arange(0,10000) / f / T \n",
    "\n",
    "if filt == 'bessel':\n",
    "    ii = 13 # case to show\n",
    "    case_to_show = 'case_' + str(ii).zfill(2)\n",
    "    cp_to_show   = [-3,-4] # -3 is p0 (stagnation point), -4 is p16(training edge point)\n",
    "    fig,ax = plt.subplots(figsize=(5,3))\n",
    "    ax.plot(t_star[0:2500],gust_origin.loc[[case_to_show]].to_numpy()[:,cp_to_show[0]],label='$C_{P}$, no filtered', color='red', linewidth=2)\n",
    "    ax.plot(t_star[0:2500],gust_origin.loc[[case_to_show]].to_numpy()[:,cp_to_show[1]], color='red', linewidth=2)\n",
    "    ax.plot(t_star[0:2500],gust.loc[[case_to_show]].to_numpy()[:,cp_to_show[0]],label='$C_{P}$, bessel filtered', color='b', linewidth=2)\n",
    "    ax.plot(t_star[0:2500],gust.loc[[case_to_show]].to_numpy()[:,cp_to_show[1]], color='b', linewidth=2)\n",
    "    ax.legend()\n",
    "    ax.margins(x=0)\n",
    "    ax.set_xlabel('$t^*$')\n",
    "    ax.set_ylabel('$C_{P}$')\n",
    "    ax.set_ylim(-4.5,6)\n",
    "    ax.grid(color='k', linestyle='-', linewidth=0.1)\n",
    "    \n",
    "elif filt == 'no filter':\n",
    "    print('No filter. No compare.')\n",
    "else:\n",
    "    print('Error. Please define your filter.')\n",
    "\n",
    "fig.savefig('CP_Filter_case_%d_port%d.png' % (ii,0), bbox_inches='tight', dpi=300)\n",
    "fig.savefig('CP_Filter_case_%d_port%d.svg' % (ii,0), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5240f83-247a-4749-a145-e11944dc2773",
   "metadata": {},
   "source": [
    "# retrieve cp cl and cd from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02190f-a0e2-4fdf-8a3b-bf437fdf31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve CP, 16(CP) + 16(CP_1st_order) + 16(CP_2nd_order) = 48 columns in total\n",
    "# simply drop CL and CD, then the rest is CP\n",
    "CP = gust.drop(['CL','CD'],axis=1, inplace=False)\n",
    "CP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20dedc6-1052-4edf-85f2-d196f1386986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve CL and CD\n",
    "CL = gust[['CL']]\n",
    "CD = gust[['CD']]\n",
    "gust[[force]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e93ee-54ad-45bc-8b16-60c83628b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve for 8 different folds\n",
    "# read the split .txt file; the number means the cases for test\n",
    "split_path = '../delta_wing_32cases_split_random.txt'\n",
    "with open(split_path, 'r') as file1:    \n",
    "    fold_split = file1.read().splitlines()       # split by \\n\n",
    "fold_split = fold_split[1:] # drop the head line\n",
    "for i in range(8):\n",
    "    fold_split[i] = fold_split[i][9:].split()    # split by space; the number starts from 10th character.\n",
    "    for j in range(4):\n",
    "        fold_split[i][j] = int(fold_split[i][j]) # convert string to int\n",
    "fold_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c371b-62bf-4bfc-b466-8c83b9a0f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the 32 case number\n",
    "case_all_num = np.arange(gust.index.nunique()) + 1\n",
    "print(case_all_num)\n",
    "print(case_all_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8160d8-c789-4114-af55-eb91d37144ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the case number for train and test cases;\n",
    "if task == 'cross_validation':\n",
    "    train_case = [ [0]*24 for _ in range(folds) ] # initialize 2d list; list for 'case_nn'\n",
    "    test_case  = [ [0]*4 for _ in range(folds) ]\n",
    "    # find case number for cross validation folds: fold1~fold7\n",
    "    for i in range(folds): \n",
    "        train_case_temp = np.setdiff1d( case_all_num, np.concatenate((fold_split[i], fold_split[-1]))) \n",
    "        # drop test case and the last one (which is for fold 8 , final test)\n",
    "        test_case_temp  = fold_split[i]\n",
    "\n",
    "        # conbine to 'case_nn' sample for train and test case; prepare for next step\n",
    "        for j in range(len(train_case_temp)):\n",
    "            train_case[i][j] = 'case_' + str(train_case_temp[j]).zfill(2)\n",
    "        for k in range(len(test_case_temp)):\n",
    "            test_case[i][k]  = 'case_' + str(test_case_temp[k]).zfill(2)\n",
    "            \n",
    "elif task == 'predict': \n",
    "    train_case = [ [0]*28 for _ in range(folds) ] # initialize 2d list; list for 'case_nn'\n",
    "    test_case  = [ [0]*4 for _ in range(folds) ]\n",
    "    # find case number for test fold: fold8  \n",
    "    train_case_temp = np.setdiff1d(case_all_num, fold_split[-1])\n",
    "    test_case_temp  = fold_split[-1]\n",
    "    for jj in range(len(train_case_temp)):\n",
    "        train_case[-1][jj] = 'case_' + str(train_case_temp[jj]).zfill(2)\n",
    "    for kk in range(len(test_case_temp)):\n",
    "        test_case[-1][kk]  = 'case_' + str(test_case_temp[kk]).zfill(2)\n",
    "\n",
    "else:\n",
    "    print('Error. Please define your task.')\n",
    "\n",
    "print(test_case[0])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a0d15-0874-42b0-8a58-42ab8248fe6a",
   "metadata": {},
   "source": [
    "#  prepare for train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668a40d-e195-4139-baf7-3692134e6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == 'cross_validation':\n",
    "    F_train_rec = np.zeros([folds,60000,1]) # 24*2500 = 60000\n",
    "    F_test_rec  = np.zeros([folds,10000,1])  # 4*2500  = 10000\n",
    "    P_train_rec = np.zeros([folds,60000,48])\n",
    "    P_test_rec  = np.zeros([folds,10000,48])\n",
    "\n",
    "    for i in range(folds): # 7 folds for training, the last one is for test, not retrieve here, since this is for training\n",
    "        if force == 'CL':\n",
    "            F_train_rec[i] = CL.loc[train_case[i]].to_numpy()\n",
    "            F_test_rec[i]  = CL.loc[test_case[i]].to_numpy()\n",
    "        elif force == 'CD':\n",
    "            F_train_rec[i] = CD.loc[train_case[i]].to_numpy()\n",
    "            F_test_rec[i]  = CD.loc[test_case[i]].to_numpy()\n",
    "        else:\n",
    "            print('Error. Please define force.')\n",
    "\n",
    "        P_train_rec[i] = CP.loc[train_case[i]].to_numpy()\n",
    "        P_test_rec[i]  = CP.loc[test_case[i]].to_numpy()\n",
    "\n",
    "elif task == 'predict':\n",
    "    F_train_rec = np.zeros([folds,70000,1]) # 28*2500 = 70000\n",
    "    F_test_rec  = np.zeros([folds,10000,1])  # 4*2500  = 10000\n",
    "    P_train_rec = np.zeros([folds,70000,16])\n",
    "    P_test_rec  = np.zeros([folds,10000,16])\n",
    "    \n",
    "    if force == 'CL':\n",
    "        F_train_rec[0] = CL.loc[train_case[0]].to_numpy()\n",
    "        F_test_rec[0]  = CL.loc[test_case[0]].to_numpy()\n",
    "    elif force == 'CD':\n",
    "        F_train_rec[0] = CD.loc[train_case[0]].to_numpy()\n",
    "        F_test_rec[0]  = CD.loc[test_case[0]].to_numpy()\n",
    "    else:\n",
    "        print('Error. Please define force.')\n",
    "\n",
    "    P_train_rec[0] = CP.loc[train_case[0]].to_numpy()\n",
    "    P_test_rec[0]  = CP.loc[test_case[0]].to_numpy()\n",
    "    \n",
    "else:\n",
    "    print('Error. Please define your task.')\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ec141-0c5b-4c30-9c90-04deb218eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task)\n",
    "print(force)\n",
    "print(folds)\n",
    "print(filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83beadb8-9f00-4941-bdcc-c59ebf85ebbe",
   "metadata": {},
   "source": [
    "# training MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10d759-7484-48df-9782-649e27f19cf4",
   "metadata": {},
   "source": [
    "## define MLP model\n",
    "- it is a sequential API\n",
    "- we can also use functional API, but here we just use sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174005fc-3bf2-417d-b300-6ddd4e0c0965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_shape, num_neurons, num_layers, weight_decay, learning_rate):\n",
    "    \"\"\"\n",
    "    Create a Multi-Layer Perceptron (MLP) model.\n",
    "    \n",
    "    :param input_shape: Shape of the input data.\n",
    "    :param num_neurons: Number of neurons in each hidden layer.\n",
    "    :param num_layers: Number of hidden layers.\n",
    "    :param weight_decay: L2 regularization factor.\n",
    "    :param learning_rate: Learning rate for the optimizer.\n",
    "    :return: Compiled MLP model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Input(shape=(input_shape,)))  \n",
    "\n",
    "    # Hidden layers\n",
    "    for _ in range(num_layers):       \n",
    "        model.add(Dense(num_neurons, kernel_regularizer=regularizers.l2(weight_decay))) \n",
    "        model.add(PReLU())\n",
    "\n",
    "    # Output layer\n",
    "    # output dimension = 1 since it is a regression problem; no activation for output layer\n",
    "    model.add(Dense(1, activation='linear', kernel_regularizer=regularizers.l2(weight_decay)))   \n",
    "\n",
    "    # Compile model\n",
    "    adam = optimizers.Adam(learning_rate=learning_rate)    \n",
    "    model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "    # for regression problems, mean squared error (MSE) is often employed; \n",
    "    # do not use 'accuracy', as this is used for classification problem\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e99f1-b468-4f82-bf84-40c4590a2297",
   "metadata": {},
   "source": [
    "## define callback func\n",
    "- to predict (not validate) using the current trained model on epoch end\n",
    "- we can define the prediction frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043384c1-d831-4153-9707-717055415a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionOnEpochEnd(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, prediction_freq=1, P_test=None):\n",
    "        \"\"\"\n",
    "        Initialize the callback.\n",
    "\n",
    "        :param prediction_freq: Frequency of epochs on which to make predictions.\n",
    "        :param P_test: Test data to predict on.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.prediction_freq = prediction_freq\n",
    "        self.P_test = P_test\n",
    "        self.epoch_predictions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        At the end of each epoch, make predictions if the epoch number is a multiple of the prediction frequency.\n",
    "\n",
    "        :param epoch: The current epoch number.\n",
    "        :param logs: Additional logs.\n",
    "        \"\"\"\n",
    "        if (epoch + 1) % self.prediction_freq == 0:\n",
    "            self.epoch_predictions.append(self.model.predict(self.P_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cff1fe-774a-4b47-b980-a72c12545161",
   "metadata": {},
   "source": [
    "## define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e18ab8-4590-496e-9d99-36179fb32de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_neuron = [72]#[16, 24, 32, 40, 48, 56, 64, 72]\n",
    "num_layer  = [8]#[1,2,3,4,5,6,7,8]\n",
    "epoch_total = 200 # 200 #l1_n8:193; l1_n16:196; l1_n24:189; l1_n32:166\n",
    "weight_decay = 0 # 5e-4 # l2 regularization hyperparameter\n",
    "learn_rate   = 1e-5 # 1e-4\n",
    "batch_how    = 200  # 200\n",
    "\n",
    "ensemble_num = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f1994-b2a1-4ae9-aa54-00f533715e6e",
   "metadata": {},
   "source": [
    "## begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67de22f-6d2e-49cb-8ef4-83d5673e65e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the data\n",
    "# define a dictionary to store the data\n",
    "history_all = {}\n",
    "predictions = {}\n",
    "\n",
    "# record start time\n",
    "starttime = datetime.datetime.now()\n",
    "# sleep 2 seconds\n",
    "time.sleep(0)\n",
    "\n",
    "for i in num_neuron: # define the neurons\n",
    "    \n",
    "    for j in num_layer: # define the hidden layers\n",
    "        \n",
    "        for k in range(ensemble_num): # start loop; if folds=7, then do cross validation; if folds=1, then, do predict\n",
    "            \n",
    "            print('start fold'+str(k+1).zfill(2)+' | layer'+str(j)+' |neuron'+str(i))\n",
    "            \n",
    "            F_train = F_train_rec[0][:,:]   # change to 2d for training\n",
    "            F_test  = F_test_rec[0][:,:]\n",
    "            P_train = P_train_rec[0][:,:]\n",
    "            P_test  = P_test_rec[0][:,:]\n",
    "            '''load data done'''\n",
    "            \n",
    "            # Create MLP model\n",
    "            model = create_mlp_model(input_shape=16, num_neurons=i, num_layers=j, weight_decay=weight_decay, learning_rate=learn_rate)\n",
    "\n",
    "            model.summary()\n",
    "\n",
    "            # Initialize callback\n",
    "            predictions[k,i,j] = PredictionOnEpochEnd(prediction_freq=1, P_test=P_test)  # Predict every 1 epoch, we can use other frequency\n",
    "            \n",
    "            # # load model, only for continuing training\n",
    "            # model = load_model('neuron_' + str(num_neuron[0]) + '_layer_' + str(num_layer[0]) + '.h5')\n",
    "            \n",
    "            '''history_all[i,j] = '''\n",
    "            model.fit(P_train, F_train, batch_size = batch_how, epochs = epoch_total, verbose = 2, shuffle=False, \n",
    "                      validation_split=0.0, callbacks=[predictions[k,i,j]]) \n",
    "            # no shuffle for time-based data; no validation, since 7-fold cross-validation already include validaiton; \n",
    "            # callbacks is used for prediction history record; \n",
    "            # no need for history_all, which records the training error history, but we only care about predict error.\n",
    "\n",
    "# record end time\n",
    "endtime = datetime.datetime.now()\n",
    "# print total time\n",
    "print('code runs %d s' % (endtime - starttime).seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c0611-0f6a-4d56-9db4-24e80e66cb32",
   "metadata": {},
   "source": [
    "# plot the mse history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125f51a-9ed6-42ca-af0b-d5acf70bce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "mse_pred_per_epoch = np.zeros([ensemble_num,epoch_total])\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "\n",
    "neuron_this = num_neuron[0]\n",
    "layer_this  = num_layer[0]\n",
    "\n",
    "for i in range(ensemble_num): # number of folds\n",
    "    for j in range(epoch_total):\n",
    "        mse_pred_per_epoch[i,j] = mse(F_test_rec[0][:][:], predictions[i,neuron_this,layer_this].epoch_predictions[j][:,0])\n",
    "    axes.semilogy(mse_pred_per_epoch[i,:], label='emsemble: %d' % (i))\n",
    "    axes.legend()\n",
    "    \n",
    "    # find the index of the min value in an array\n",
    "    aaa = mse_pred_per_epoch[i].tolist()\n",
    "    min_value = min(aaa)\n",
    "    min_value_index = aaa.index(min_value)\n",
    "    print('The index of the min value is: %d | The min value is: %f' % (min_value_index, min_value))\n",
    "# axes[0].set_xlim([0,50])\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Mean Square Error (log)')\n",
    "axes.set_title('Neuron:%d, Layer:%d || weight_decay:%.5f || learn_rate:%f || batch_size:%d' % (neuron_this,layer_this, weight_decay, learn_rate,batch_how))\n",
    "# axes.text(165, 0.1, 'elu')\n",
    "\n",
    "# axes.semilogy(history_all[num_neuron[0],num_layer[0]].history['loss'], label='train error')\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef6e87-a26e-49ed-9ceb-03e9698ae610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function finds the ensemble average of the epoch predictions and the std  history\n",
    "# it returns the average result (array) and the std (array)\n",
    "def ensemble_average(epoch_pred):\n",
    "    sum_temp = np.zeros((len(epoch_pred[:,0]),len(epoch_pred[0,:])))\n",
    "    sum_temp[:,0] = epoch_pred[:,0]\n",
    "    std_hist = np.zeros((len(epoch_pred[0,:])))\n",
    "    std_hist[0] = np.std(sum_temp[:,0])\n",
    "    ensemble_averg = np.zeros((len(epoch_pred[:,0]),len(epoch_pred[0,:])))\n",
    "    ensemble_averg[:,0] = epoch_pred[:,0]\n",
    "    for i in range(len(epoch_pred[0,:])-1):\n",
    "        sum_temp[:,i+1] = sum_temp[:,i] + epoch_pred[:,i+1]\n",
    "        std_hist[i+1] = np.std(sum_temp[:,i+1]/(i+2))\n",
    "        ensemble_averg[:,i+1] = sum_temp[:,i+1]/(i+2)\n",
    "    return(std_hist, ensemble_averg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a1d1f-363f-4b4e-baf7-280bc3b8d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function ensemble_average\n",
    "a = np.array([[1,1,2,2],[1,2,1,3],[1,1,1,5]])\n",
    "(b,c) = ensemble_average(a)\n",
    "print(c)\n",
    "print(b == np.std(np.array([[1,2/2,4/3,6/4],[1,3/2,4/3,7/4],[1,2/2,3/3,8/4]]),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7aa02-d8fb-4ead-8b40-944762e35b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell calcalute the mse of different ensembling average: 50,60,70,80,90,...200\n",
    "\n",
    "ensemble_use = np.arange(50,210,10)\n",
    "mse_ensemble_use  = {} # dictionary, with keys and values\n",
    "epoch_hist_use    = np.zeros([len(ensemble_use),len(F_test_rec[0]),ensemble_num]) # 16*10000*30: 16 to show the mse, 10000 samples, 30 repeats\n",
    "ensemble_avrg_use = {}\n",
    "ensemble_std_use  = {}\n",
    "ensemble_avrg_use = {}\n",
    "\n",
    "fig,ax_use = plt.subplots(figsize=(5,3))\n",
    "for j in ensemble_use: # [50,60,70,80,90,...200]\n",
    "    for i in range(ensemble_num): # repeat ensemble_num times with same hyperparameters\n",
    "        epoch_hist_use[int(j/10-5),:,i] = predictions[i,neuron_this,layer_this].epoch_predictions[j-1][:,0]\n",
    "        \n",
    "    (ensemble_std_use[j], ensemble_avrg_use[j]) = ensemble_average(epoch_hist_use[int(j/10-5)])\n",
    "    \n",
    "    mse_ensemble_use[j] = mse(ensemble_avrg_use[j][:,-1], F_test_rec[0][:,:])\n",
    "\n",
    "ax_use.plot(mse_ensemble_use.keys(),mse_ensemble_use.values(),color='blue', linewidth=1.5,marker='.',markersize=8)\n",
    "ax_use.set_xlim(50,200)\n",
    "ax_use.set_xticks(np.arange(50,210,30).tolist())\n",
    "ax_use.set_ylim(0.010,0.026)\n",
    "ax_use.grid(color='k', linestyle='-', linewidth=0.1)\n",
    "ax_use.set_xlabel('Epochs')\n",
    "ax_use.set_ylabel('MSE')\n",
    "fig.savefig('ensemble_error_Filter_neuron%d_layer%d.png' % (neuron_this,layer_this), bbox_inches='tight', dpi=300)\n",
    "fig.savefig('ensemble_error_Filter_neuron%d_layer%d.svg' % (neuron_this,layer_this), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c61ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "ax.plot(ensemble_std_use[200],color='blue', linewidth=1.5,marker='.',markersize=8)\n",
    "ax.grid(color='k', linestyle='-', linewidth=0.1)\n",
    "ax.set_xlabel('Number of Repeats')\n",
    "ax.set_ylabel('STD')\n",
    "ax.margins(x=0)\n",
    "ax.set_xlim(0,30)\n",
    "fig.savefig('STD_repeat_Filter_neuron%d_layer%d.png' % (neuron_this,layer_this), bbox_inches='tight', dpi=300)\n",
    "fig.savefig('STD_repeat_Filter_neuron%d_layer%d.svg' % (neuron_this,layer_this), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "fig, ax_split = plt.subplots(2,2,figsize=(8,4))\n",
    "repeat_num = 200 # repeated number: 50 100\n",
    "\n",
    "T = 0.3 # period is 0.3s\n",
    "f = 1000 # frequency is 1000Hz\n",
    "t_star = (np.arange(0,10000)-500) / f / T \n",
    "\n",
    "fig_num = 0 # for count opnly\n",
    "for i in range(4):\n",
    "    if fig_num in [0,1]:\n",
    "        p = 0\n",
    "        q = fig_num\n",
    "    else:\n",
    "        p = 1\n",
    "        q = fig_num - 2\n",
    "    ax_split[p,q].plot(t_star[0:2500-1], F_test_rec[0][2500*i:2500*(i+1)-1,:],label='F_test',color='red', linewidth=2)\n",
    "    ax_split[p,q].plot(t_star[0:2500-1], ensemble_avrg_use[repeat_num][2500*i:2500*(i+1)-1,-1],label='pred_average', \n",
    "                       color='blue', linewidth=2, linestyle='--') # 50 and 200 epochs\n",
    "    ax_split[p,q].grid(color='k', linestyle='-', linewidth=0.1)\n",
    "    ax_split[p,q].set_xticks(np.arange(0,9))\n",
    "    ax_split[p,q].set_xlim(0,8)\n",
    "    ax_split[p,q].margins(x=0)\n",
    "    ax_split[p,q].set_xlabel('$t^*$')\n",
    "    fig_num = fig_num + 1\n",
    "ax_split[0,0].set_ylabel('$C_{L}$')\n",
    "ax_split[0,0].legend(['$C_{L}$, measured','$C_{L}$, predicted'])\n",
    "ax_split[1,0].set_ylabel('$C_{L}$')\n",
    "\n",
    "fig.savefig('ensemble%d_pred_Filter_neuron%d_layer%d.png' % (repeat_num,neuron_this,layer_this), bbox_inches='tight', dpi=300)\n",
    "fig.savefig('ensemble%d_pred_Filter_neuron%d_layer%d.svg' % (repeat_num,neuron_this,layer_this), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c0f9e-4cac-4e8f-ad4e-042d5ea75492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "num_neuron = [72]#[16, 24, 32, 40, 48, 56, 64, 72]\n",
    "num_layer  = [6]#[1,2,3,4,5,6,7,8]\n",
    "model = load_model('neuron_' + str(num_neuron[0]) + '_layer_' + str(num_layer[0]) + 'case20_best' '.h5')\n",
    "# check, plot\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "T = 0.3 # period is 0.3s\n",
    "f = 1000 # frequency is 1000Hz\n",
    "t_star = np.arange(0,10000) / f / T \n",
    "fig, axes = plt.subplots(1,1,figsize=(12, 5))\n",
    "\n",
    "neuron_this = num_neuron[0]\n",
    "\n",
    "layer_this  = num_layer[0]\n",
    "\n",
    "# axes.set_ylim([-0.15,0.35])\n",
    "# axes.set_ylim([9.8,10.4])\n",
    "axes.plot(t_star, F_test_rec[0][:,:],label='F_test',color='red', linewidth=3)\n",
    "axes.plot(t_star, model.predict(P_test_rec[0][:,:]), label='F_pred',color='blue', linewidth=2, linestyle='--')\n",
    "axes.legend()\n",
    "axes.set_xlabel('T$^*$')\n",
    "axes.set_ylabel('CL')\n",
    "axes.set_title('Fold08 Neuron: %d, Layer:%d || Mean_error:%.5f' % (neuron_this,layer_this, mse(F_test_rec[0][:,:],model.predict(P_test_rec[0][:,:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aafac5-01d7-4ab2-92d5-d9d8101b89e8",
   "metadata": {},
   "source": [
    "# save data as csv: cl without filter; cl with bessel filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ab8c3-d96b-48c5-8f6e-e5893c5eca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_filter   = gust.drop(['CL','CD'],axis=1, inplace=False)\n",
    "CP_nofilter = gust_origin.drop(['CL','CD'],axis=1, inplace=False)\n",
    "\n",
    "# CL_true_CL_no_filter = F_test_rec[0][:,:]\n",
    "# CL_predit_CP_filter_bessel = model.predict(P_test_rec[0][:,:])\n",
    "\n",
    "# t_star = pd.DataFrame(data=t_star).to_numpy() # convert t_start form 1d to 2d\n",
    "# predict_CL_no_filter_CP_filter_bessel = np.concatenate((t_star, CL_true_CL_no_filter, CL_predit_CP_filter_bessel), axis=1)\n",
    "\n",
    "# predict_CP_filter_bessel = pd.DataFrame(data=predict_CL_no_filter_CP_filter_bessel, columns=['t_star','CL_true_CL_no_filter','CL_predit_CP_filter_bessel'])\n",
    "# predict_CP_filter_bessel.to_csv('predict_CP_filter_bessel.csv')\n",
    "\n",
    "CP_plot_map = pd.DataFrame(data=np.concatenate((CP_nofilter,CP_filter), axis=1), index=gust_index, columns=[item for item in ['CP_nofilter'] for _ in range(16)]+\n",
    "                                                                                            [item for item in ['CP_filter'] for _ in range(16)])\n",
    "CP_plot_map.to_csv('CP_plot_map.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa4e708-b1de-4543-a9b5-5504b46ac56c",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65db913-40f2-4315-b0d9-71e951e04a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('p_filter_neuron_' + str(num_neuron[0]) + '_layer_' + str(num_layer[0]) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ae923-03db-483e-8207-3eda971dd352",
   "metadata": {},
   "source": [
    "# save prediction history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0a024-a568-4891-8f92-501dbeddb6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# #Save the variable\n",
    "# pickle.dump(predictions, open(\"predictions.dat\", \"wb\"))\n",
    "# #Load the variable\n",
    "# variable = pickle.load(open(\"predictions.dat\", \"rb\"))\n",
    "\n",
    "for i in num_neuron: # define the neurons\n",
    "    for j in num_layer: # define the hidden layers \n",
    "        for k in range(ensemble_num): # start loop for 7 folds cross-validation\n",
    "            pickle.dump(predictions[k,i,j].epoch_predictions, open('F_pred_p_filter_ensemble_num_'+str(k)+'_neuron_'+str(i)+'_layer_'+str(j)+'.dat', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec0cac-96bb-4ee4-ae8c-50aefe209406",
   "metadata": {},
   "source": [
    "# create gif to illustrate the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdc410-7845-4879-85eb-2cb4ba3e0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''create Gif'''\n",
    "# check, plot\n",
    "T = 0.3 # period is 0.3s\n",
    "f = 1000 # frequency is 1000Hz\n",
    "t_star = np.arange(0,10000) / f / T \n",
    "fig, axes = plt.subplots(1,1,figsize=(12, 5))\n",
    "\n",
    "neuron_this = num_neuron[0]\n",
    "layer_this  = num_layer[0]\n",
    "j = 0 # for count only\n",
    "for i in range(0,min_value_index,20):\n",
    "    axes.set_ylim([-1, 3])\n",
    "    axes.plot(t_star, F_test_rec[0][:,:] / shift_for_relu, label='F_test',color='red', linewidth=3)\n",
    "    axes.plot(t_star, predictions[0,neuron_this,layer_this].epoch_predictions[i][:,0] / shift_for_relu, label='F_pred',color='blue', linewidth=2, linestyle='--')\n",
    "    axes.legend()\n",
    "    axes.set_xlabel('T$^*$')\n",
    "    axes.set_ylabel('Plane Normal Force Coefficient')\n",
    "    axes.set_title('Neuron: %d, Layer: %d, Epoch: %d' % (neuron_this,layer_this, i))\n",
    "    fig.savefig('./image_n32_l1/predict_neuron32_layer1_epoch_%d_lift.png' % (i), bbox_inches='tight', dpi=300)\n",
    "    axes.cla()\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b9dbe-2fbb-4dc8-a293-02fbd3df6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Gif\n",
    "image_name = []\n",
    "for i in range(j):\n",
    "    image_name.append('predict_neuron32_layer1_epoch_%d_lift.png' % (i*20))\n",
    "import imageio\n",
    "with imageio.get_writer('./image_n32_l1/mygif_lift.gif', mode='I') as writer:\n",
    "    for filename in image_name:\n",
    "        image = imageio.imread('./image_n32_l1/' +  filename)\n",
    "        writer.append_data(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "51d70ca6f67856079a32dd433dc7b2f235041bb42b697569d4b18a5049359686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
